{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Experiment System:\n",
    "GPU: NVIDIA GeForce RTX 3090 24G;\n",
    "CPU: 14 vCPU Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz;\n",
    "RAM: 46G;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from fvcore.common.timer import Timer\n",
    "\n",
    "from utils import *\n",
    "from models.kan.LBFGS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch, logger, start_index):\n",
    "    model.train()\n",
    "    fvctimer = Timer()\n",
    "    closure_count_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, start_index):\n",
    "        closure_count = 0\n",
    "\n",
    "        data, target = todevice(data, device), todevice(target, device)\n",
    "\n",
    "        if fvctimer.is_paused():\n",
    "            fvctimer.resume()\n",
    "        else:\n",
    "            fvctimer.reset()\n",
    "\n",
    "        if args.optimizer == \"adam\":\n",
    "            # print(\"adam\")\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            if args.loss == \"cross_entropy\":\n",
    "                losses = [F.cross_entropy(output, target)]\n",
    "            elif args.loss == \"mse\":\n",
    "                losses = [F.mse_loss(output, target)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            loss = 0\n",
    "            for l in losses:\n",
    "                loss = loss + l\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        elif args.optimizer == \"lbfgs\":\n",
    "            # print(\"lbfgs\")\n",
    "\n",
    "            def closure():\n",
    "                nonlocal closure_count\n",
    "                closure_count += 1\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                if args.loss == \"cross_entropy\":\n",
    "                    losses = [F.cross_entropy(output, target)]\n",
    "                elif args.loss == \"mse\":\n",
    "                    losses = [F.mse_loss(output, target)]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                loss = 0\n",
    "                for l in losses:\n",
    "                    loss = loss + l\n",
    "\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        fvctimer.pause()\n",
    "        closure_count_list.append(closure_count)\n",
    "\n",
    "    total_training_time = fvctimer.seconds()\n",
    "    average_training_time_per_iteration = fvctimer.avg_seconds()\n",
    "    total_closure_count = sum(closure_count_list)\n",
    "    average_closure_count_per_iteration = numpy.mean(closure_count_list)\n",
    "    print(f\"Epoch: {epoch}; total training time: {total_training_time:,} seconds; average training time per iteration: {average_training_time_per_iteration:,} seconds\")\n",
    "    print(f\"Epoch: {epoch}; total closure count: {total_closure_count}; average closure count per iteration: {average_closure_count_per_iteration}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    randomness_control(args.seed)\n",
    "\n",
    "    train_loader, test_loader, num_classes, input_size = get_loader(args, use_cuda = use_cuda)\n",
    "\n",
    "    args.output_size = num_classes\n",
    "    args.input_size = input_size\n",
    "\n",
    "    args.activation = get_activation(args)\n",
    "    args.kan_shortcut_function = get_shortcut_function(args)\n",
    "\n",
    "    model = get_model(args)\n",
    "    model = model.to(device)\n",
    "    num_parameters, flops = get_model_complexity(model, None, args)\n",
    "\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "    elif args.optimizer == \"lbfgs\":\n",
    "        optimizer = LBFGS(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            lr=args.lr, \n",
    "            history_size=10, \n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            tolerance_grad=1e-32, \n",
    "            tolerance_change=1e-32, \n",
    "            tolerance_ys=1e-32)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "    print(f\"Model: {args.model}, Number of parameters: {num_parameters:,}, FLOPs: {flops:,}\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch, logger = None, start_index = (epoch - 1) *len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_once(args, model, device, train_loader, optimizer, epoch, logger, start_index):\n",
    "    model.train()\n",
    "    fvctimer = Timer()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, start_index):\n",
    "        data, target = todevice(data, device), todevice(target, device)\n",
    "\n",
    "        if fvctimer.is_paused():\n",
    "            fvctimer.resume()\n",
    "        else:\n",
    "            fvctimer.reset()\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        fvctimer.pause()\n",
    "\n",
    "    total_forward_time = fvctimer.seconds()\n",
    "    average_forward_time_per_iteration = fvctimer.avg_seconds()\n",
    "    print(f\"Epoch: {epoch}; total forward time: {total_forward_time:,} seconds; average forward time per iteration: {average_forward_time_per_iteration:,} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_forward(args):\n",
    "\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    randomness_control(args.seed)\n",
    "\n",
    "    train_loader, test_loader, num_classes, input_size = get_loader(args, use_cuda = use_cuda)\n",
    "\n",
    "    args.output_size = num_classes\n",
    "    args.input_size = input_size\n",
    "\n",
    "    args.activation = get_activation(args)\n",
    "    args.kan_shortcut_function = get_shortcut_function(args)\n",
    "\n",
    "    model = get_model(args)\n",
    "    model = model.to(device)\n",
    "    num_parameters, flops = get_model_complexity(model, None, args)\n",
    "\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = None\n",
    "        pass\n",
    "    elif args.optimizer == \"lbfgs\":\n",
    "        optimizer = LBFGS(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            lr=args.lr, \n",
    "            history_size=10, \n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            tolerance_grad=1e-32, \n",
    "            tolerance_change=1e-32, \n",
    "            tolerance_ys=1e-32)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "    print(f\"Model: {args.model}, Number of parameters: {num_parameters:,}, FLOPs: {flops:,}\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        forward_once(args, model, device, train_loader, optimizer, epoch, logger = None, start_index = (epoch - 1) *len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN\n",
    "Restart the notebook before evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kan_args():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "\n",
    "    parser.add_argument('--model', type=str, default=\"KAN\", #required=True,\n",
    "                        help='network structure')\n",
    "    parser.add_argument('--layers_width', type=int, default=[8, 8], nargs='+', #required=True,\n",
    "                        help='the width of each hidden layer')\n",
    "    parser.add_argument('--batch_norm', action='store_true', default=False,\n",
    "                        help='whether use batch normalization')\n",
    "    parser.add_argument('--activation_name', type=str, default=\"relu\", \n",
    "                        help='activation function')\n",
    "    parser.add_argument('--pre_train_ckpt', type=str, default=\"\", \n",
    "                        help='path of the pretrained model')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, default=\"MNIST\", #required=True,\n",
    "                        help='dataset')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=1024,\n",
    "                        help='input batch size for training (default: 1024)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=128,\n",
    "                        help='input batch size for testing (default: 128)')\n",
    "    parser.add_argument('--epochs', type=int, default=3, # 100 MNIST pretrain, 5 Finetune\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--optimizer', type=str, default=\"lbfgs\",\n",
    "                        help='supported optimizer: adam, lbfgs')\n",
    "    # parser.add_argument('--gamma', type=float, default=0.7,\n",
    "    #                     help='Learning rate step gamma (default: 0.7, 1.0 for fewshot)')\n",
    "    parser.add_argument('--loss', type=str, default=\"cross_entropy\",\n",
    "                        help='loss function')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1314,\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100000,\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--save-model-interval', type = int, default=-1, \n",
    "                        help='whether save model along training')\n",
    "    parser.add_argument('--evaluateion_interval', type = int, default=100000, \n",
    "                        help='interval between two evaluations')\n",
    "    ################# Parameters for KAN #################\n",
    "    parser.add_argument('--kan_bspline_grid', type=int, default=15, \n",
    "                        help='the grid size of the bspline in the KAN layer')\n",
    "    parser.add_argument('--kan_bspline_order', type=int, default=3, \n",
    "                        help='the order of the bspline in the KAN layer')\n",
    "    parser.add_argument('--kan_shortcut_name', type=str, default=\"silu\", \n",
    "                        help='the shortcut(base) function in the KAN layer: zero, identity, silu')\n",
    "    parser.add_argument('--kan_grid_range', type=float, default=[-4, 4], nargs=2,\n",
    "                        help='the range of the grid in the KAN layer. default is [-1, 1]. but for general normalized data, it can be larger.')\n",
    "    ################# Parameters for KAN #################\n",
    "    ################# Parameters for MLP #################\n",
    "    ## pass ##\n",
    "    ################# Parameters for MLP #################\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.save_model_along = args.save_model_interval > 0\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n",
      "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
      "Epoch: 1; total training time: 77.31417033076286 seconds; average training time per iteration: 1.3104096666230993 seconds\n",
      "Epoch: 1; total closure count: 1535; average closure count per iteration: 26.016949152542374\n",
      "Epoch: 2; total training time: 75.79276225715876 seconds; average training time per iteration: 1.2846230891043857 seconds\n",
      "Epoch: 2; total closure count: 1534; average closure count per iteration: 26.0\n",
      "Epoch: 3; total training time: 75.65926801413298 seconds; average training time per iteration: 1.282360474815813 seconds\n",
      "Epoch: 3; total closure count: 1530; average closure count per iteration: 25.93220338983051\n"
     ]
    }
   ],
   "source": [
    "main(get_kan_args())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
    "Epoch: 1; total training time: 77.31417033076286 seconds; average training time per iteration: 1.3104096666230993 seconds\n",
    "Epoch: 1; total closure count: 1535; average closure count per iteration: 26.016949152542374\n",
    "Epoch: 2; total training time: 75.79276225715876 seconds; average training time per iteration: 1.2846230891043857 seconds\n",
    "Epoch: 2; total closure count: 1534; average closure count per iteration: 26.0\n",
    "Epoch: 3; total training time: 75.65926801413298 seconds; average training time per iteration: 1.282360474815813 seconds\n",
    "Epoch: 3; total closure count: 1530; average closure count per iteration: 25.93220338983051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "Restart the notebook before evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_args():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "\n",
    "    parser.add_argument('--model', type=str, default=\"MLP\", #required=True,\n",
    "                        help='network structure')\n",
    "    parser.add_argument('--layers_width', type=int, default=[1024, 1024], nargs='+', #required=True,\n",
    "                        help='the width of each hidden layer')\n",
    "    parser.add_argument('--batch_norm', action='store_true', default=False,\n",
    "                        help='whether use batch normalization')\n",
    "    parser.add_argument('--activation_name', type=str, default=\"relu\", \n",
    "                        help='activation function')\n",
    "    parser.add_argument('--pre_train_ckpt', type=str, default=\"\", \n",
    "                        help='path of the pretrained model')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, default=\"MNIST\", #required=True,\n",
    "                        help='dataset')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=1024,\n",
    "                        help='input batch size for training (default: 1024)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=128,\n",
    "                        help='input batch size for testing (default: 128)')\n",
    "    parser.add_argument('--epochs', type=int, default=3, # 100 MNIST pretrain, 5 Finetune\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--optimizer', type=str, default=\"lbfgs\",\n",
    "                        help='supported optimizer: adam, lbfgs')\n",
    "    # parser.add_argument('--gamma', type=float, default=0.7,\n",
    "    #                     help='Learning rate step gamma (default: 0.7, 1.0 for fewshot)')\n",
    "    parser.add_argument('--loss', type=str, default=\"cross_entropy\",\n",
    "                        help='loss function')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1314,\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100000,\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--save-model-interval', type = int, default=-1, \n",
    "                        help='whether save model along training')\n",
    "    parser.add_argument('--evaluateion_interval', type = int, default=100000, \n",
    "                        help='interval between two evaluations')\n",
    "    ################# Parameters for KAN #################\n",
    "    parser.add_argument('--kan_bspline_grid', type=int, default=20, \n",
    "                        help='the grid size of the bspline in the KAN layer')\n",
    "    parser.add_argument('--kan_bspline_order', type=int, default=5, \n",
    "                        help='the order of the bspline in the KAN layer')\n",
    "    parser.add_argument('--kan_shortcut_name', type=str, default=\"silu\", \n",
    "                        help='the shortcut(base) function in the KAN layer: zero, identity, silu')\n",
    "    parser.add_argument('--kan_grid_range', type=float, default=[-4, 4], nargs=2,\n",
    "                        help='the range of the grid in the KAN layer. default is [-1, 1]. but for general normalized data, it can be larger.')\n",
    "    ################# Parameters for KAN #################\n",
    "    ################# Parameters for MLP #################\n",
    "    ## pass ##\n",
    "    ################# Parameters for MLP #################\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.save_model_along = args.save_model_interval > 0\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n",
      "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
      "Epoch: 1; total training time: 5.6864582523703575 seconds; average training time per iteration: 0.09638064834526029 seconds\n",
      "Epoch: 1; total closure count: 1525; average closure count per iteration: 25.847457627118644\n",
      "Epoch: 2; total training time: 4.047977037727833 seconds; average training time per iteration: 0.06860978030047174 seconds\n",
      "Epoch: 2; total closure count: 1522; average closure count per iteration: 25.796610169491526\n",
      "Epoch: 3; total training time: 4.682358644902706 seconds; average training time per iteration: 0.07936201093055434 seconds\n",
      "Epoch: 3; total closure count: 1535; average closure count per iteration: 26.016949152542374\n"
     ]
    }
   ],
   "source": [
    "main(get_mlp_args())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
    "Epoch: 1; total training time: 5.6864582523703575 seconds; average training time per iteration: 0.09638064834526029 seconds\n",
    "Epoch: 1; total closure count: 1525; average closure count per iteration: 25.847457627118644\n",
    "Epoch: 2; total training time: 4.047977037727833 seconds; average training time per iteration: 0.06860978030047174 seconds\n",
    "Epoch: 2; total closure count: 1522; average closure count per iteration: 25.796610169491526\n",
    "Epoch: 3; total training time: 4.682358644902706 seconds; average training time per iteration: 0.07936201093055434 seconds\n",
    "Epoch: 3; total closure count: 1535; average closure count per iteration: 26.016949152542374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Time Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n",
      "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
      "Epoch: 1; total forward time: 4.250635847449303 seconds; average forward time per iteration: 0.07204467538049665 seconds\n",
      "Epoch: 2; total forward time: 2.2687958478927612 seconds; average forward time per iteration: 0.03845416691343663 seconds\n",
      "Epoch: 3; total forward time: 2.29159764200449 seconds; average forward time per iteration: 0.0388406380000761 seconds\n"
     ]
    }
   ],
   "source": [
    "main_forward(get_kan_args())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
    "Epoch: 1; total forward time: 4.250635847449303 seconds; average forward time per iteration: 0.07204467538049665 seconds\n",
    "Epoch: 2; total forward time: 2.2687958478927612 seconds; average forward time per iteration: 0.03845416691343663 seconds\n",
    "Epoch: 3; total forward time: 2.29159764200449 seconds; average forward time per iteration: 0.0388406380000761 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n",
      "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
      "Epoch: 1; total forward time: 1.757744014263153 seconds; average forward time per iteration: 0.029792271428189035 seconds\n",
      "Epoch: 2; total forward time: 0.024965673685073853 seconds; average forward time per iteration: 0.0004231470116114212 seconds\n",
      "Epoch: 3; total forward time: 0.02934861183166504 seconds; average forward time per iteration: 0.0004974340988417803 seconds\n"
     ]
    }
   ],
   "source": [
    "main_forward(get_mlp_args())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
    "Epoch: 1; total forward time: 1.757744014263153 seconds; average forward time per iteration: 0.029792271428189035 seconds\n",
    "Epoch: 2; total forward time: 0.024965673685073853 seconds; average forward time per iteration: 0.0004231470116114212 seconds\n",
    "Epoch: 3; total forward time: 0.02934861183166504 seconds; average forward time per iteration: 0.0004974340988417803 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
      "Epoch: 1; total training time: 4.801859557628632 seconds; average training time per iteration: 0.08138745012929884 seconds\n",
      "Epoch: 1; total closure count: 0; average closure count per iteration: 0.0\n",
      "Epoch: 2; total training time: 2.7768291011452675 seconds; average training time per iteration: 0.04706490001941131 seconds\n",
      "Epoch: 2; total closure count: 0; average closure count per iteration: 0.0\n",
      "Epoch: 3; total training time: 2.7728042006492615 seconds; average training time per iteration: 0.04699668136693663 seconds\n",
      "Epoch: 3; total closure count: 0; average closure count per iteration: 0.0\n"
     ]
    }
   ],
   "source": [
    "kan_args = get_kan_args()\n",
    "kan_args.optimizer = \"adam\"\n",
    "main(kan_args)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: KAN, Number of parameters: 134,762, FLOPs: 3,545,632.0\n",
    "Epoch: 1; total training time: 4.801859557628632 seconds; average training time per iteration: 0.08138745012929884 seconds\n",
    "Epoch: 1; total closure count: 0; average closure count per iteration: 0.0\n",
    "Epoch: 2; total training time: 2.7768291011452675 seconds; average training time per iteration: 0.04706490001941131 seconds\n",
    "Epoch: 2; total closure count: 0; average closure count per iteration: 0.0\n",
    "Epoch: 3; total training time: 2.7728042006492615 seconds; average training time per iteration: 0.04699668136693663 seconds\n",
    "Epoch: 3; total closure count: 0; average closure count per iteration: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1314\n",
      "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
      "Epoch: 1; total training time: 1.297071099281311 seconds; average training time per iteration: 0.02198425592002222 seconds\n",
      "Epoch: 1; total closure count: 0; average closure count per iteration: 0.0\n",
      "Epoch: 2; total training time: 0.15516074001789093 seconds; average training time per iteration: 0.0026298430511506937 seconds\n",
      "Epoch: 2; total closure count: 0; average closure count per iteration: 0.0\n",
      "Epoch: 3; total training time: 0.16446852684020996 seconds; average training time per iteration: 0.002787602149834067 seconds\n",
      "Epoch: 3; total closure count: 0; average closure count per iteration: 0.0\n"
     ]
    }
   ],
   "source": [
    "mlp_args = get_mlp_args()\n",
    "mlp_args.optimizer = \"adam\"\n",
    "main(mlp_args)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Output:\n",
    "seed 1314\n",
    "Model: MLP, Number of parameters: 1,863,690, FLOPs: 3,725,322\n",
    "Epoch: 1; total training time: 1.297071099281311 seconds; average training time per iteration: 0.02198425592002222 seconds\n",
    "Epoch: 1; total closure count: 0; average closure count per iteration: 0.0\n",
    "Epoch: 2; total training time: 0.15516074001789093 seconds; average training time per iteration: 0.0026298430511506937 seconds\n",
    "Epoch: 2; total closure count: 0; average closure count per iteration: 0.0\n",
    "Epoch: 3; total training time: 0.16446852684020996 seconds; average training time per iteration: 0.002787602149834067 seconds\n",
    "Epoch: 3; total closure count: 0; average closure count per iteration: 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanbefair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
